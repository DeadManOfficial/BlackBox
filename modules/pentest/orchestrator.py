"""
Main Orchestrator for DeadMan Pen.

Autonomous penetration testing orchestrator that coordinates:
- Decision engine (if/then rules)
- Goal tracking
- Attack path execution
- State management
- Real-time intel integration
"""

import asyncio
import yaml
import logging
from pathlib import Path
from dataclasses import dataclass, field
from typing import Any, Callable, Dict, List, Optional
from datetime import datetime

from .state import (
    StateManager, Finding, AccessLevel, Severity, AssetType,
    Asset, AccessRecord, ToolExecution as ToolExecRecord, Decision
)
from .decision import DecisionEngine, AssessmentContext, PhaseTransition
from .goals import GoalTracker, GoalStatus
from .paths import PathRouter, AttackPath, PathResult, PathStatus
from .workers import WorkerPool, ToolTask, TaskPriority, RateLimitConfig, Scope, TimeWindow
from .mcp_bridge import MCPToolBridge, ToolResult
from .parser import ResultParser, ParsedResult
from .intel import IntelFetcher
from .bounty import BountyTracker


def _get_default_data_dir() -> Path:
    """Get cross-platform default data directory."""
    import os
    data_dir = Path(os.environ.get('BLACKBOX_DATA_DIR', Path.home() / '.blackbox' / 'pentest'))
    data_dir.mkdir(parents=True, exist_ok=True)
    return data_dir


@dataclass
class OrchestratorConfig:
    """Configuration for the orchestrator."""
    db_path: str = field(default_factory=lambda: str(_get_default_data_dir() / "state.db"))
    config_path: str = field(default_factory=lambda: str(_get_default_data_dir() / "config.yaml"))
    max_workers: int = 10
    rate_limit_rps: float = 10.0
    safe_mode: bool = True
    auto_save_interval: int = 60
    verbose: bool = True


@dataclass
class AssessmentResult:
    """Final result of an assessment."""
    target: str
    status: str
    duration_seconds: float
    phases_completed: List[str]
    findings: List[Finding]
    access_achieved: AccessLevel
    assets_acquired: List[Asset]
    goals_summary: Dict[str, Any]
    bounty_matches: List[str]


class Orchestrator:
    """
    Main orchestrator for autonomous penetration testing.

    Coordinates all components:
    - Decision engine for if/then rules
    - Goal tracker for multi-goal tracking
    - Path router for parallel attack execution
    - Worker pool for tool execution
    - State manager for persistence
    - Intel fetcher for real-time enrichment
    """

    def __init__(self, config: OrchestratorConfig = None):
        """Initialize the orchestrator."""
        self.config = config or OrchestratorConfig()
        self._setup_logging()
        self._load_config()
        self._init_components()

    def _setup_logging(self):
        """Setup logging."""
        log_config = self._file_config.get('logging', {}) if hasattr(self, '_file_config') else {}

        self.logger = logging.getLogger('deadman-pen')
        self.logger.setLevel(getattr(logging, log_config.get('level', 'INFO')))

        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                log_config.get('format', '%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)

    def _load_config(self):
        """Load configuration from YAML."""
        config_path = Path(self.config.config_path)
        if config_path.exists():
            with open(config_path) as f:
                self._file_config = yaml.safe_load(f)
        else:
            self._file_config = {}

    def _init_components(self):
        """Initialize all components."""
        # State manager
        self.state = StateManager(self.config.db_path)

        # MCP tool bridge
        self.tools = MCPToolBridge()

        # Result parser
        self.parser = ResultParser()

        # Decision engine
        self.decision = DecisionEngine(self.state, self._file_config)

        # Goal tracker
        self.goals = GoalTracker(self.state, self._file_config)

        # Path router
        self.paths = PathRouter(self.state, self.tools, self.parser, self._file_config)

        # Intel fetcher
        self.intel = IntelFetcher(self.tools)

        # Bounty tracker
        self.bounty = BountyTracker()

        # Worker pool
        rate_limit = RateLimitConfig(
            requests_per_second=self.config.rate_limit_rps,
            burst_size=int(self.config.rate_limit_rps * 2)
        )
        self.workers = WorkerPool(
            max_workers=self.config.max_workers,
            rate_limit=rate_limit
        )
        self.workers.set_tool_executor(self._execute_tool)

        # Assessment state
        self._current_assessment_id: Optional[int] = None
        self._running = False
        self._paused = False

    async def _execute_tool(self, tool: str, params: Dict[str, Any]) -> Dict[str, Any]:
        """Execute a tool via MCP bridge."""
        result = await self.tools.execute(tool, params)
        return result.data if result.success else {'error': result.error}

    async def run(self, target: str, resume: bool = False,
                  goals: Dict[str, Any] = None) -> AssessmentResult:
        """
        Run autonomous penetration test.

        Args:
            target: Target domain or URL
            resume: Resume previous assessment if exists
            goals: Custom goal configuration

        Returns:
            AssessmentResult with findings and status
        """
        start_time = datetime.now()
        self._running = True

        try:
            # Initialize or resume assessment
            if resume:
                assessment_state = self.state.get_assessment_state(target)
                if assessment_state:
                    self._current_assessment_id = assessment_state.id
                    self.logger.info(f"Resuming assessment {assessment_state.id} from phase {assessment_state.current_phase}")
                else:
                    self.logger.info(f"No previous assessment found for {target}, starting new")
                    self._current_assessment_id = self._create_assessment(target)
            else:
                self._current_assessment_id = self._create_assessment(target)

            self.logger.info(f"Starting assessment of {target} (ID: {self._current_assessment_id})")

            # Main assessment loop
            phases_completed = []
            max_iterations = 100  # Safety limit to prevent infinite loops
            iteration = 0
            last_phase = None

            while self._running and not self._paused:
                iteration += 1
                if iteration > max_iterations:
                    self.logger.warning(f"Max iterations ({max_iterations}) reached - stopping")
                    break
                # Build context
                ctx = self.decision.build_context(self._current_assessment_id)

                # Check if assessment should continue
                if not self.decision.should_continue(ctx):
                    self.logger.info("Assessment stopping - should_continue returned False")
                    break

                # Check goal completion
                if self.goals.is_assessment_complete(self._current_assessment_id):
                    self.logger.info("All goals achieved - assessment complete")
                    break

                # Get next actions
                actions = self.decision.get_next_actions(ctx)

                # Log current state
                self.logger.info(f"Phase {ctx.current_phase} - Access: {ctx.access_level.name}")

                # Execute phase
                phase_result = await self._execute_phase(ctx, actions)

                if phase_result.get('phase_completed'):
                    phases_completed.append(ctx.current_phase)

                # Handle transitions
                transitions = actions['transitions']
                if transitions:
                    best_transition = transitions[0]
                    if best_transition.to_phase != ctx.current_phase:
                        self.logger.info(f"Transitioning: {ctx.current_phase} -> {best_transition.to_phase}")
                        self.state.update_assessment(
                            self._current_assessment_id,
                            current_phase=best_transition.to_phase
                        )

                        # Log decision
                        self.decision.log_decision(
                            self._current_assessment_id,
                            rule_name='phase_transition',
                            condition_met=best_transition.reason,
                            action_taken=f"Transition to {best_transition.to_phase}",
                            context_before={'phase': ctx.current_phase},
                            context_after={'phase': best_transition.to_phase}
                        )

                    # Handle parallel branches
                    parallel_transitions = [t for t in transitions if t.parallel]
                    if parallel_transitions:
                        self.logger.info(f"Launching {len(parallel_transitions)} parallel branches")
                        # Execute parallel branches asynchronously
                        # (In production, would spawn separate workers)

                # Auto-save
                await self._auto_save()

            # Finalize assessment
            self.state.complete_assessment(self._current_assessment_id)

            # Build result
            duration = (datetime.now() - start_time).total_seconds()
            return self._build_result(target, phases_completed, duration)

        except Exception as e:
            self.logger.error(f"Assessment failed: {e}")
            raise

        finally:
            self._running = False

    def _create_assessment(self, target: str) -> int:
        """Create new assessment."""
        # Extract domain from target
        from urllib.parse import urlparse
        parsed = urlparse(target if '://' in target else f'https://{target}')
        domain = parsed.netloc or target

        return self.state.create_assessment(
            target=target,
            domain=domain,
            config={
                'safe_mode': self.config.safe_mode,
                'started_at': datetime.now().isoformat(),
            }
        )

    async def _execute_phase(self, ctx: AssessmentContext,
                             actions: Dict[str, Any]) -> Dict[str, Any]:
        """Execute a single phase of the assessment."""
        result = {
            'phase': ctx.current_phase,
            'phase_completed': False,
            'findings_count': 0,
            'tools_executed': 0,
        }

        # Get tools to execute
        tools = actions.get('tools', [])
        paths = actions.get('paths', [])

        # Convert to tasks
        tasks = []

        for tool_exec in tools:
            if hasattr(tool_exec, 'tool'):
                tasks.append(ToolTask(
                    id=f"{tool_exec.tool}_{len(tasks)}",
                    tool=tool_exec.tool,
                    params=tool_exec.params,
                    priority=TaskPriority.NORMAL
                ))

        # Execute tools via worker pool
        if tasks:
            self.logger.info(f"Executing {len(tasks)} tools")
            task_results = await self.workers.execute_batch(tasks)
            result['tools_executed'] = len(task_results)

            # Process results
            for task_result in task_results:
                if task_result.status.value == 'completed':
                    await self._process_tool_result(ctx, task_result)

        # Execute attack paths
        if paths:
            viable_paths = self.paths.discover_paths(self._context_to_dict(ctx))
            if viable_paths:
                self.logger.info(f"Executing {len(viable_paths)} attack paths")
                path_results = await self.paths.execute_parallel(viable_paths[:5])  # Limit to 5

                for path_result in path_results:
                    await self._process_path_result(ctx, path_result)

        # Check findings
        findings = self.state.get_findings(self._current_assessment_id)
        result['findings_count'] = len(findings)
        result['phase_completed'] = True

        return result

    async def _process_tool_result(self, ctx: AssessmentContext,
                                   task_result) -> None:
        """Process a tool execution result."""
        # Log execution
        self.state.log_tool_execution(
            self._current_assessment_id,
            ToolExecRecord(
                phase=ctx.current_phase,
                tool=task_result.tool,
                params={},  # Task result doesn't include params
                result=task_result.data,
                duration_ms=task_result.duration_ms
            )
        )

        # Parse result
        parsed = self.parser.parse(task_result.tool, task_result.data, ctx.current_phase)

        # Save findings
        for finding in parsed.findings:
            finding_id = self.state.save_finding(self._current_assessment_id, finding)
            self.logger.info(f"Finding: {finding.title} [{finding.severity.value}]")

            # Enrich with intel
            enriched = await self.intel.enrich_finding(finding.__dict__)

            # Match to bounty targets
            bounty_match = self.bounty.match_finding_to_target(finding.__dict__)
            if bounty_match:
                self.bounty.add_finding_to_target(bounty_match, finding.__dict__)
                self.logger.info(f"  -> Matched bounty target: {bounty_match}")

        # Save assets
        for asset in parsed.assets:
            self.state.save_asset(self._current_assessment_id, asset)
            self.logger.info(f"Asset acquired: {asset.name} [{asset.type.value}]")

        # Save access records
        if parsed.access_granted:
            self.state.save_access_level(
                self._current_assessment_id,
                AccessRecord(
                    level=parsed.access_granted,
                    context=task_result.tool,
                    credentials={}
                )
            )
            self.logger.info(f"Access granted: {parsed.access_granted.name}")

    async def _process_path_result(self, ctx: AssessmentContext,
                                   path_result: PathResult) -> None:
        """Process an attack path result."""
        self.logger.info(f"Path {path_result.path_id}: {path_result.status.value}")

        # Save findings from path
        for finding in path_result.findings:
            self.state.save_finding(self._current_assessment_id, finding)

        # Save access gained
        if path_result.access_gained:
            self.state.save_access_level(
                self._current_assessment_id,
                AccessRecord(
                    level=path_result.access_gained,
                    context=path_result.path_id,
                    credentials={}
                )
            )

        # Log decision
        self.decision.log_decision(
            self._current_assessment_id,
            rule_name=f"path_{path_result.path_id}",
            condition_met=f"Path execution: {path_result.status.value}",
            action_taken=f"Steps: {path_result.steps_completed}/{path_result.total_steps}",
            context_before={'path': path_result.path_id},
            context_after={
                'findings': len(path_result.findings),
                'access': path_result.access_gained.name if path_result.access_gained else None
            }
        )

    def _context_to_dict(self, ctx: AssessmentContext) -> Dict[str, Any]:
        """Convert context to dictionary for path router."""
        return {
            'target': ctx.target,
            'target_url': ctx.target_url or f"https://{ctx.target}",
            'domain': ctx.domain,
            'access_level': ctx.access_level,
            'tech_stack': ctx.tech_stack,
            'endpoints': ctx.endpoints,
            'js_bundles': ctx.js_bundles,
            'subdomains': ctx.subdomains,
            'jwt_tokens': ctx.jwt_tokens,
            'oauth_endpoints': ctx.oauth_endpoints,
            'ai_endpoints': ctx.ai_endpoints,
            'supabase_url': ctx.supabase_url,
            'firebase_project': ctx.firebase_project,
            'has_backend_access': ctx.has_backend_access,
            'has_backend_direct_access': ctx.has_backend_direct_access,
            'checkout_endpoint': ctx.checkout_endpoint,
            'webhook_endpoint': ctx.webhook_endpoint,
            'user_endpoint': ctx.user_endpoint,
            'user_id_params': ctx.user_id_params,
            'extracted_keys': ctx.extracted_keys,
            'findings': ctx.findings,
            'vulns_found': ctx.vulns_found,
        }

    async def _auto_save(self) -> None:
        """Periodic auto-save."""
        # State is automatically saved via StateManager
        # This is for any additional save operations
        pass

    def _build_result(self, target: str, phases_completed: List[str],
                      duration: float) -> AssessmentResult:
        """Build final assessment result."""
        assessment = self.state.get_assessment(self._current_assessment_id)
        findings = self.state.get_findings(self._current_assessment_id)
        access_records = self.state.get_access_records(self._current_assessment_id)
        assets = self.state.get_assets(self._current_assessment_id)

        highest_access = self.state.get_highest_access(self._current_assessment_id)
        goals_summary = self.goals.get_summary(self._current_assessment_id)

        # Find bounty matches
        bounty_matches = []
        for target_obj in self.bounty.targets:
            if target_obj.findings:
                bounty_matches.append(target_obj.target_asset)

        return AssessmentResult(
            target=target,
            status=assessment['status'] if assessment else 'unknown',
            duration_seconds=duration,
            phases_completed=phases_completed,
            findings=findings,
            access_achieved=highest_access or AccessLevel.ANONYMOUS,
            assets_acquired=assets,
            goals_summary=goals_summary,
            bounty_matches=bounty_matches
        )

    def pause(self) -> None:
        """Pause the assessment."""
        self._paused = True
        self.logger.info("Assessment paused")

    def resume(self) -> None:
        """Resume paused assessment."""
        self._paused = False
        self.logger.info("Assessment resumed")

    def stop(self) -> None:
        """Stop the assessment."""
        self._running = False
        self.logger.info("Assessment stopped")

    def get_status(self) -> Dict[str, Any]:
        """Get current assessment status."""
        if not self._current_assessment_id:
            return {'status': 'no_assessment'}

        assessment = self.state.get_assessment(self._current_assessment_id)
        goals = self.goals.get_summary(self._current_assessment_id)

        return {
            'assessment_id': self._current_assessment_id,
            'target': assessment['target'] if assessment else None,
            'status': assessment['status'] if assessment else 'unknown',
            'current_phase': assessment['current_phase'] if assessment else None,
            'running': self._running,
            'paused': self._paused,
            'goals': goals,
            'findings_count': len(self.state.get_findings(self._current_assessment_id)),
            'access_level': self.state.get_highest_access(self._current_assessment_id).name
                           if self.state.get_highest_access(self._current_assessment_id) else 'ANONYMOUS',
        }

    def export_report(self, path: str, format: str = 'json') -> None:
        """Export assessment report."""
        if not self._current_assessment_id:
            raise ValueError("No active assessment")

        if format == 'json':
            self.state.export_json(self._current_assessment_id, path)
        else:
            raise ValueError(f"Unsupported format: {format}")


async def main():
    """CLI entry point."""
    import sys

    if len(sys.argv) < 2:
        print("Usage: python -m engine.orchestrator <target> [--resume]")
        sys.exit(1)

    target = sys.argv[1]
    resume = '--resume' in sys.argv

    config = OrchestratorConfig(verbose=True)
    orchestrator = Orchestrator(config)

    try:
        result = await orchestrator.run(target, resume=resume)

        print("\n" + "=" * 60)
        print("ASSESSMENT COMPLETE")
        print("=" * 60)
        print(f"Target: {result.target}")
        print(f"Status: {result.status}")
        print(f"Duration: {result.duration_seconds:.1f}s")
        print(f"Phases: {', '.join(result.phases_completed)}")
        print(f"Access: {result.access_achieved.name}")
        print(f"Findings: {len(result.findings)}")
        print(f"Assets: {len(result.assets_acquired)}")

        if result.bounty_matches:
            print(f"\nBounty Matches: {', '.join(result.bounty_matches)}")

        print("\nGoals:")
        for goal, status in result.goals_summary.get('goal_status', {}).items():
            print(f"  {goal}: {status['status']} ({status['percentage']:.0f}%)")

    except KeyboardInterrupt:
        print("\nAssessment interrupted")
        orchestrator.stop()


if __name__ == '__main__':
    asyncio.run(main())
